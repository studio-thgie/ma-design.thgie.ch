
# Voice assistants as assistive echnologies

With [Person DgaB8Q](data/people/Person%20DgaB8Q.md)

---

00:07 I: Du konntest ja schon viel Erfahrung mit Sprachassistenten sammeln. Möchtest du mir ein wenig davon erzählen, wie es dazu kam und was dich daran interessiert hat?

00:24 P: Interessiert hat es mich eigentlich von Anfang an. Wie sowas überhaupt möglich ist, wie es funktioniert. Und wie es ist um mit einer AI zu sprechen. Das war dann aber eher ernüchternd. Das man nur so kurze Kommandos geben kann im wesentlichen. Und das sie es oft nicht versteht. Beruflich hatte ich im Zusammenhang einer Med-Tech Firma damit zu tun. Da ging es darum, dass man so ein Gerät, welches Medikamente appliziert, später fragen kann, ob man ein Medikament schon genommen hat. Da habe ich gelernt, anstatt nur Amazon Services abzufragen, wie man auch ein IoT Device einbinden kann. Das du nachher Alexa fragen, ich nehme jetzt ein Beispiel, fragst du das Aquarium, ob du den Fischen schon Futter gegeben hast. Das brauche ich heute auch beim Unterrichten, um zu erklären wie Alexa funktioniert, als Anschauungsbeispiel.

01:52 I: Da hattest du mir auch Beispiele davon geschickt, ein Vortrag oder so etwas ähnliches.

02:02 P: Die ganze Vorlesung ist online. Kannst du gut auch nochmals nachsehen.

02:14 P: Es ist ja auch nicht so schwierig. Im Wesentlichen wird jedes Mal ein Webhook getriggert oder ausgelöst. Und wenn du diesen mit einem Webservice beantworten kannst, dann kannst du eigentlich beeinflussen, was die Alexa sagt. Das ist eigentlich das fasznierende, dass die ganze Komplexität sich auf dem Backend von Amazon abspielt. Wenn das besser wird, dann versteht Alexa plötzlich besser was ich will, aber meine App macht immer noch genau das selbe. Ist immer noch genau einfach. Aber Alexa lernt in dieser Zeit von allen Haushalten, mit welchen sie verbunden ist. Irgendwie finde ich das noch faszinierend als technisches System.

03:13 I: Das ist in diesem Sinne ja auch wahnsinnig cybernetisch. Es gibt einen riesigen Feedback-Loop bei welchem wir alle mit eingebunden sind.  

03:24 P: Ja, wenn man vom Menschen her kommt, dann geht man schon davon aus, dass das Gegenüber etwas lernt von einem Gespräch. Aber bei der Alexa lernt Alexa ja von allen Gesprächen welche sie parallel führt, dauernd. Das ist sehr schwierig sich vorzustellen. Mal angenommen, dass sie wirklich so gebaut ist, dass sie lernt. Vielleicht trainiert sie auch nur und ist nachher ein trainiertes Model, welches eingesetzt wird und dann wärend dem reden nicht mehr lernt. Aber theoretisch möglich wäre es wahrscheinlich. Das sie dauernd am lernen ist.

04:02 I: Du hattest gesagt, dass du anfangs enttäuscht warst, von den Möglichkeiten, von der Beschränkung auf kurze Kommandos. Wie war das später, diese AI auch programmieren zu können?

04:23 P: Dann beginnst du zu verstehen, warum sie so einfach sein muss. Weil sie im wesentlichen versucht, solche intents, also Absichten zu extrahieren, aus dem Gesagten. Wenn du selber solche Absichten einsetzen möchtest musst du dem Sytem Äusserungen, also Utterancen, mitgeben. Also Beispielsätze, welche die Absicht ausdrücken. Ich mache nur Prototyping. Von dem her weiss ich nicht wie schwierig es ist eine wirklich gute Sprachapp zu machen. Welche danach wirklich funktionieren würde für die User.

05:11 I: Ich habe persönlich das Gefühl, dass sich das in den nächsten 5 bis 10 Jahren noch wahnsinnig weiterentwickeln wird. Jetzt ist ja das GPT-3 herausgekommen vor einigen Monaten. Ich glaube IBM, IBM Watson, hat ja auch eine Analyse Platform welche sehr stark ist. Da bin ich sehr gespannt wie das in den nächsten 5 bis 10 Jahren daher kommt. 

05:42 P: Ich weiss nicht ob diese generativen Modele, das gleiche sind, oder was diese für die Alexa bedeuten. Ob das Verständnis der Absicht besser wird gleichzeitig mit diesen generativen Modelen, oder ob das etwas separates ist.  

06:03 I: Ich kann mir vorstellen, also eine Funktionalität welche du über GPT abbilden kannst, ist Zusammenfassungen. Also du kannst einen grossen Text zusammenfassen, in ein Abstrakt. Das würde ja dann ermöglichen, dass grössere oder längere Sätze, ganze Gesprächsfetzen, versucht werden runterzubrechen auf etwas was die Maschine dann auffassen könnte. Mal so in den Raum geworfen

06:38 I: In deiner Arbeit mit Amazon Echo, hattest du auch das Gefühl, das ist gefährlich oder da hat es wie ein Risiko, im Umgang oder der Applikation von Sprachassistenten? Sache welche du kritisch betrachten würdest.

07:02 P: Allgemein ist es halt für die Privatsphäre ein potentielles Problem, wenn man ein vernetztes Mikrofon in der Wohnung hat. Unabhängig wie gut dieses Model ist. Weil halt jemand anders in dieses Mikrofon reinhören kann. Dazu gibt es verschiedene Beispiele, dass Mitarbeiter von Amazon zum Training bei Gesprächen zugehört haben. Das Alexa aus Versehen auslöst und nicht merkt dass sie zuhört. Ich glaube, es muss nicht mal eine böse Absicht dahinter sein von Amazon. Das kann trotzdem ein Problem sein. Oder dieser Fall in welchem aufgrund eines DSGVO Antrages falsche Daten zurückgegeben worden sind, von jemand anderem. Dort hat ja Amazon vermutlich versucht, dass richtige zu tun und es kam trotzdem falsch raus. Ich glaube wenn man millionen von solchen Geräten hat, dann passieren auch die unwahrscheinlichen Sachen irgendwann. Von dem her ist es dann für einzelne Leute ein Problem. Darum glaube ich, bei uns in Europe oder wenigstens in Deutschland und Schweiz, also in den Ländern in welchen Privatsphäre höher gewertet wird, gibt es auch weniger Menschen die solche Produkte brauchen. Ich hab es zum Beispiel zuhause nicht, aber mich interessiert es trotzdem, und auch was überhaupt möglich ist. Jetzt sieht man auch das Alexa für Business angeboten wird, für Hotels und sogar für Spitäler. Und das ist ja dann ja wieder je nach Kontext noch viel schlimmer. Gerade im Spital, wenn ich nicht alleine im Zimmer bin, hört sie ja auch noch meinem Zimmerpartner zu und solche Dinge, bei welchen man ja noch weniger Kontrolle hat. Es geht dauernd um kritische Sachen, bei welchen man nicht will, dass das andere Personen wissen.  

09:35 I: Ich denke ein Punkt ist, dass Amazon in der Schweiz nicht wirklich präsent ist. Amazon pusht diese Echo und Alexa Technologien schon sehr fescht. Die ist ja mittlerweile ja in allen Geräten welche über Amazon produziert oder vermarktet werde. Von Fernsehern, Smart Clocks, whatever.

10:00 I: Ich hatte heute noch ein interessantes Interview mit dem SBV, dem Schweizerischen Verband für Blinde. Sie haben beobachtet, dass Siri auf dem iPhone wahnsinnig beliebt ist, weil es einfach out-of-the-box funktioniert. Du kannst das iPhone anschalten und es tut. Sie haben aber of tdas Feedback erhalten, dass Amazon Echo oder Google Home Devices noch Hürden in der Aufsetzung darstellen. Sie haben zum Beispiel selber einen Test gemacht. Dabei musste sie für ein Amazon Echo Device ein Captcha auf einem iPhone lösen und konnten das nicht, weil es nur ein visuelles Captcha gab und kein Audio Captcha. Sie sind also gar nicht durch den ganzen Setup Prozess gelangt.

11:02 P: Das Sprachsteuerung an sich willkommen wäre, aber sie kommen gar nicht dazu. Das ist noch interessant.

11:13 I: Sie haben dann gesagt. Die Produkte von Apple welche schon auf den Laptops oder iPhone funktionieren, zum Beispiel das VoiceOver, welches sehr beliebt ist, als genereller Screen Reader, der ist so schnell eingeschalten, nur ein Häckchen in den Preferenzen. Diese Produkte sind sehr beliebt.  

11:39 P: Das sind halt auch eine völlig andere Kategorie von Produkten. Die Sprachsynthese ist schon ewig lange gelöst. Sprache verstehen ist schon etwas anderes.

11:50 I: Und genaus aus diesem Grund sie Sprachassistenten oft für kleinere Tasks, wie zum Beispiel eine App öffnen oder den Hotspot aktivieren, wird dann gerne Siri gebraucht.  

12:04 P: Siri läuft jetzt glaube ich zum Teil auch wirklich auf dem Phone, mit diesem machine-learning Prozessor den sie jetzt haben.

12:11 I: Das es also gar nicht mehr über?

12:14 P: Das nicht mehr alles übertragen werden muss. Wenn du ja etwas vom Telefon selber möchtest, ist die Domaine so eingeschränkt, dass es viel einfacher ist als allgemein verstehen zu wollen, was ich jetzt will.

12:29 P: Ich sage immer "Alexa", aber das ist ja nur weil es sozusagen die Kategorie definiert hat und nicht... Mir ist es völlig egal ob es um Amazon geht oder eine andere Firma welche das gleiche macht. Es ist einfach das prototypische Beispiel.

12:45 I: Sie machen es ja schon auch sehr viel einfacher, um damit zu entwickeln.

12:53 P: Ja, es gibt so ein ganzes Ökosystem, welches ähnlich wie das App-Ökosystem ist. Aber ist trotzdem separat. Es hat enorm gute Ressourcen inzwischen um auf verschiedenen Abstraktionsleveln solche Apps zu kreiieren. Man kann von Templates ausgehen, für gewüsse Domainen gibt es solche Hilfsobjekte, zum Beispiel Home Automation, welche schon wissen was eine Lampe ist und ein Lichtschalter. Inzwischen kannst du Sprachapps auch besser direkt einbinden. Sonst musstest du immer sagen: "Alexa, ask Fischtank" und nacher eine Frage stellen. Um überhaupt zu meiner App zu kommen. Jetzt kann ich solche, weiss nicht mehr genau wie es heisst, nameless invocation oder so. Ich kann also solche Beispielsätze geben, auch für das Starten der App. Dann ist das natürlich zum Starten, dann muss ich mich nicht immer zuerst an Alexa wenden und ihr sagen, ob sie den Fischtank etwas fragen könne. Das ist ja auch unnatürlich.

14:02 I: So gut! Du hattest mal erwähnt, glaube in einer Email, oder einen Kommentar zu Design von Konversationen gemacht. Das dich das noch interessieren würd. Konntest du da Erfahrungen sammeln, jetzt in deiner eigenen Arbeit oder im Kurs den du gibst. Wie so eine Konversation gestaltet wird, gibt es ja einerseits eine technische Ebene, wo Probleme gelöst werden müssen. Aber es gibt dann auch, was du eben angesprochen hast, es ist natürlicher zu sagen: "Hey Fischtank, hab ich meine Fische schon gefüttert?" anstatt "Alexa, frag Fischtank". Konntest du da Erfahrungen sammeln mit dem Design von solchen Konversationen?

14:52 P: Das einzige was mir aufgefallen ist in letzter Zeit, ist, dass sie eine Art Session-Konzept hat. Wenn man mit Alexa spricht neuerdings. Vorher war es immer nur ein Request. Jetzt ist es wie eine Session, welche eine Zeit lang mitläuft und wahrscheinlich bewahrt sie da ein wenig Kontext auf, was man schon vorher gemacht hatte, in diesem Zusammenhang des einen Gespräches welches man am führen ist. Aber sonst, hab ich mich nicht so damit befasst. Ich habe einfach gesehen, dass es mehr solche Tools gibt und mehr Material zu diesem Thema. Das wäre eben eine grössere Sache um sich da einzuarbeiten. So wie das App Ökosystem jetzt auch etwas eigenes geworden ist. So wie, Web Entwicklung und App Entwicklung ost oberflächlich gesehen das gleiche, es ist Software, aber es ist trotzdem eine eigene Welt. Bei Voice Apps wird das auch so werden.

16:02 I: Das mit dem Session-Model schau ich mir gerne mal an. Generell finde ich es relativ spannend. Aber mir ist auch bewusst, wie schnell dass dann eine Komplexität wächst. Mit jedem Satz oder Befehl, welcher dazu kommt, wächst natürlich die Komplexität exponentiell. Weil einfach soviel Informationen vorhanden sind, welche verwaltet werden müssen, mit verschiedenen Zusammenhängen.

16:25 P: Ja, ein Stück weit. Wenn ich zum Beispiel eine Reise buchen möchte, dann ist ja klar was ich für Inputs haben muss. Und wenn sie diese Absicht erkennt hat, dann fragt sie halt nacher solange nach, bis sie alle dieser Inputs hat. Es ist ein wenig so gemacht.  

16:41 I: Dann ist es eher so ein multi-step Formular.

16:45 P: Ja, wenn ich sage: "Alexa, ich möchte in die Ferien", dann sagt sie zuerst mal, wohin möchtest du und nacher wann möchtest du. So schrittweise kommt sie immer näher. Und wenn du sagst: "Alex, buch mir morgen einen Flug nach London", dann weiss sie dass alles gleich in einem Satz.

17:03 I: Konntest du schon Erfahrungen sammeln, mit Hilfttechnologien wie Screen Reader oder Spracherkennungssoftware? Hast du mit diesen mal rumgespielt?

17:15 P: Ich hatte einen Studenten, welcher nichts sieht, und für in eine Prüfung angepasst. Damit er sie überhaupt lesen kann. Die Prüfung war sonst einfach ein PDF. Er hatte ein Textfile bevorzugt und in diesem Textfile musste es ASCII-mässige Marker haben. Diese konnte ich wählen, aber sie müssten konsistent sein. Damit konnte er in den Aufgaben rumspringen. Vom Interface her, dass er hatte, dieser Reader ist einfach wahnsinnig schnell. Uns würde es wahrscheinlich schlecht wenn wir diesem Reader zuhören würden. Für ihn war aber das einfach die Ansicht. Was mich extrem erstaunt hatte, wie fest diese Person ein Bild vom Source Code im Kopf hat. Was genau dem entspricht was ich sehe wenn ich drauf schaue, aber es ist ja trotzdem nicht das gleiche. Sie muss es so sequentiell reinladen und wir können einfach auf einen Blick die Struktur sehen.

18:33 [irrelevant, Interviewer erzählt persönliche Geschichte]

19:02 P: Man kann vielleicht sagen, dass das Gehirn eine gewissen Bandbreite hat. Wenn du dann kein Bild hast, hast du mehr frei für Ton, so zu sagen.

19:23 I: Bist du aktuell noch mit Voice Assistants am arbeiten, nebst der Schule? Oder hast du das Gefühl, dass du in naher Zukunft mal etwas machen möchtest?

19:34 P: Im Moment sind diese Sachen eher durch so Kollaboration der Fachhochscule mit der Industrie getrieben. Wenn jemand etwas in diese Richtung braucht, dann würde ich das gerne wieder ansehen. Im Moment sieht das nicht danach aus. Es ist ja auch oft bei diesen Sachen so, wenn man es sich am Anfang mal ansieht, dann hat man ein gutes Bild davon, wie es ungefähr funktioniert. Dann bleibt es meistens ein paar Jahre ähnlich. Es wird immer ausgefeilter, aber man hat trotzdem eine Idee davon, wie es funktioniert. Vielleicht gibt es irgendwann dann mal noch einen grossen Schritt, wie bei der App Programmierung vor und nach Android, oder vor dem iPhone und nach dem iPhone. Das ist ein riesen Unterschied. Dann ist diese Welt völlig neu geworden. Vielleicht gibt es so etwas auch bei den Sprach Apps, aber jetzt im Moment ist bis jetzt eigentlich gleich geblieben von den Grundkonzepten her. Darum habe ich einfach das Gefühl, ich verstehe worum es im Moment geht. Ich werde nicht Voice App Profi, oder! Bis es diese gibt bin ich mal noch gut informiert und dann gibt es ja dann diese Leute die das besser können. So wie es jetzt App und Webdesigner gibt.

20:58 I: Da hast du sozusagen eine gesunde Übersicht verschafft.  

21:03 P: Ja, um einfach grundsätzlich mal zu verstehen um was es da geht.

21:08 I: Wenn du sagst, es gibt solche grössen Sprünge, was wäre denn für dich ein wünschenswerter grösser Sprung in der Weiterentwicklung von solchen Sprachassistenz-Technologien? Wenn du spekulieren dürftest, völlig frei.

21:27 P: Im Moment ist mir da zu wenig bewusst was die Probleme sind. Klar, natürlicher Sprechen wäre ein Ziel. Das wäre bei diesen general AI am Schluss das Ziel. Das ich mit einem Gerät sprechen kann wie mit einem Mensch. Weiss jetzt nicht ob das so wichtig ist, dass das schnell Realität wird. Wie hiessen die schon wieder, Berg, früher haben sie Schultze und Web geheissen. Das ist ein Design Büro. Die hatten mal das be as smart as a puppy Prinzip. Wenn ein Gerät smart ist, muss es nicht gleich smart sein wie eine andere Person. Es reicht wenn es so smart ist wie ein junger Hund, welchem wir auch gewisse Fehler verzeihen. Solange ungefähr etwas dabei heraus springt was für uns auch ein Vorteil ist. Von dem her, wäre das eigentlich ganz ok, wenn diese ganzen Geräte im Haus wenigstens so smart wären und so harmlos.

22:49 I: Find ich super, so harmlos wie ein Puppy.