<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <base href="/"/>
    <title>Prototyping I</title>

    <meta name="generator" content="pandoc" />
            <meta name="date" content="2021-10-22" />
    
    <link rel="stylesheet" href="/assets/templates/archive.css">

</head>
<body>

    <nav role="navigation">
        <ul class="nav-h">
            <li><a target="_self" href="/">Home</a></li>
            <li><a target="_self" href="/Journal.html">Journal</a></li>
            <li><a target="_self" href="/Glossary.html">Glossary</a></li>
            <li><a target="_self" href="/Research Data.html">Data</a></li>
            <li><a target="_self" href="/Findings and Output.html">Findings and Output</a></li>
            <li><a target="_self" href="/Project Context.html">Context</a></li>
        </ul>

    </nav>

    <main role="main">
        <h1 id="prototyping-i">Prototyping I</h1>
        <p>Ausgehend von der <a
        href="output/Thematic%20Analysis.html">Thematic Analysis</a> vor
        dem zweiten Colloquium wollte ich im letzten Semester erste
        Entwicklungen mit den eigenen Ansätzen bezüglich
        Sprachassistenten umsetzen. Dazu galt es einerseits eine
        Entwicklungsumgebung (Tech Stack) aufzubauen, welche Rapid
        Prototyping erlaubt und andererseits zu definieren, wie diese
        ersten Prototypen genau zu funktionieren haben und wie sie
        testbar werden. Parallele Entwicklungen <a
        href="Vorbereitung%20und%20Zusammenfassung%20Gedanken%20f%C3%BCr%20C3">bezüglich
        der Erkundung der sozialen/Relevanz</a> haben den Prototypen
        dann noch einen zusätzlichen Dreh gegeben.</p>
        <h2 id="prototypes">Prototypes</h2>
        <p>Folge Prototypen konnten umgesetzt werden.</p>
        <h3 id="timers">Timers</h3>
        <p><em>artyom.js</em> Timer stellen ist gefühlt das
        meistgebrauchte Feature in Bezug auf Sprachassistenten. Ein
        guter Test um sich das jeweilige Framework einzuarbeiten…</p>
        <h3
        id="proximity-sensor-und-flip-switches-with-microbit">Proximity
        Sensor und Flip Switches with micro:bit</h3>
        <p><em>Mycroft, Raspberry Pi und micro:bit</em> Diese beiden
        Tests kam aus dem Gedanken heraus, dass Peripherie-Geräte der
        Interaktion mit Sprachassistenten im Bezug auf der Verhandlung
        von Privatsphäre zudienlich sein können.</p>
        <p>Die Tests waren durchaus spannend, wenn auch sehr technisch.
        Ich konnte vor allem Proof of Concepts umsetzen diese aber nicht
        eingehend testen. Beim Proximity Sensor ging es darum, dass ich
        mit Hilfe eines Beacon Objektes dem Sprachassistenten mitteile,
        ob ich mich in der Nähe befinde oder nicht. Dieser reagiert dann
        dementsprechend darauf wenn ich mich nähere und ist ansonsten
        nicht aktiv.</p>
        <p>Beim Flip Switch lässt sich mit einer kurzen Handbewegung,
        dem Umkehren eines Beacon Objektes, der Zustand des
        Sprachassistenten verändern. Der Zustand wird dann wiederum im
        Beacon Objekt wiedergegeben.</p>
        <figure>
        <img src="/assets/images/thumbs/IMG_0439.jpeg" alt="Code" />
        <figcaption aria-hidden="true">Code</figcaption>
        </figure>
        <h3 id="eliza-on-mycroft-and-artyom">ELIZA; on mycroft and
        artyom</h3>
        <p><em>Mycroft, Raspberry Pi, artyom.js</em> ELIZA war ein von
        Joseph Weizenbaum entworfenes Chatprogram, welches entlang der
        Linie einer klassischen Sprech-Therapiesitzung gestaltet wurde.
        Eigentlich wollte Weizenbaum vorführen, wie einfach der Mensch
        zu täuschen ist. Jedoch war sein Program sehr beliebt. ELIZA
        beruht auf wenigen Regeln, wie ein etwas vom Mensch
        geschriebenes zu interpretieren ist und reagiert darauf in der
        Regel mit Fragen. Ich habe ELIZA in zwei verschiedenen
        Sprachassistenz-Frameworks umgesetzt um damit die Dialogizität
        zu testen.</p>
        <h3 id="memory-holders">Memory Holders</h3>
        <p><em>artyom.js</em> Ausgehend aus einem nicht verfolgtem Thema
        der thematischen Analyse, <a
        href="output/themes/Knowledge%20Practices.html">Knowledge
        Practices</a>, wollte ich einen Sprachassistenten testen,
        welcher sich Praxen rund um Wissen oder zumindest Notizen
        fokusiert. In diesem Test lässt sich in einem Dialog mit dem
        Sprachassistenten treten und er speichert forlaufend alles
        gesagte und spiegelt es wieder.</p>
        <figure>
        <img
        src="/assets/images/thumbs/Screenshot_2021-10-24_at_11.11.50.png"
        alt="Code" />
        <figcaption aria-hidden="true">Code</figcaption>
        </figure>
        <h2 id="reflexion">Reflexion</h2>
        <p>Das letzte Semester war überraschend kurz, was ich so nicht
        antizipiert hatte. Dementsprechend war auch die Planung den
        Umständen nicht entsprechend. Ich hätte gerne mehr Zeit fürs
        Testing aufwenden können.</p>
        <p>Eines der wichtigsten Take-Aways ist wohl wie weit die
        technischen Möglichkeiten und auch Zugänglichkeit geschritten
        ist. Das ist vor allem für ein Prototyping innerhalb des Fokuses
        von Sprachassistenten sehr hilfreich und praktisch. Beim viel
        wichtigeren Teil des Testings habe ich festgestellt, dass ich
        umbedingt eine stabile Gruppe von Test-Personen brauche,
        innerhalb dessen ein Vergleich der einzelnen Tests möglich ist.
        Ansonsten gehe ich Gefahr in die <em>I-Methodology</em> zu
        fallen; Der Entwicklung und Testing aus meiner eigenen Sicht, zu
        meinen eigenen Bedürfnissen. Der Aufbaue einer solchen
        Test-Gruppe ist allerdings mit sehr viel Resourcen und Zeit
        verbunden.</p>
        <p>Ein sehr spannender Aspekt, welcher durch das Prototyping
        dazu gekommen ist, ist das <a
        href="https://neohelden.com/de/blog/conversational-interfaces/conversational-design-prinzipien/">Konversation
        Design</a>. Es geht also darum, nicht einer einfachen
        Befehl-Befolgung Struktur zu folgen, sondern jene Personen,
        welche mit den Sprachassistenten sprechen auch auf der Ebene des
        Konversation abzuholen. Dies ist ein Bereich welcher im Moment
        noch sehr schwer abzudecken ist. Es ist aber auch jener Bereich
        in welchem ich die Aushandlung von Privatsphäre platzieren
        würde.</p>
        <h2 id="tech-stack">Tech Stack</h2>
        <ul>
        <li>Mycroft <a
        href="https://mycroft.ai/">https://mycroft.ai/</a></li>
        <li>AIY Voicekit <a
        href="https://aiyprojects.withgoogle.com/voice/">https://aiyprojects.withgoogle.com/voice/</a></li>
        <li>Artyom.js <a
        href="https://sdkcarlos.github.io/sites/artyom.html">https://sdkcarlos.github.io/sites/artyom.html</a></li>
        <li>Rasperry Pi 4 <a
        href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/">https://www.raspberrypi.com/products/raspberry-pi-4-model-b/</a></li>
        <li>BBC micro:bit <a
        href="https://microbit.org/">https://microbit.org/</a> - Kleines
        Mikroprozessor-Entwicklungsboard welches mit verschiedenen
        Sensoren, zB</li>
        </ul>
        <h2 id="repositories">Repositories</h2>
        <p>Mein erarbeiteter Code ist in folgenden Repositories zu
        finden. - <a
        href="https://github.com/thgie/tele-bits">https://github.com/thgie/tele-bits</a>
        - <a
        href="https://github.com/thgie/eliza-skill">https://github.com/thgie/eliza-skill</a>
        - <a
        href="https://github.com/thgie/The-Relevance-of-Speaking-with-Things">https://github.com/thgie/The-Relevance-of-Speaking-with-Things</a></p>
    </main>

    <footer class="text-sm">
        <p class="flex">
        </p>
        <p>All content, where not noted otherwise on <a href="https://dissertation.thgie.ch">ma-design.thgie.ch</a> is released and distributed under the <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">CC-BY-SA 4.0 license</a>.</p>
    </footer>

    <script>

        document.addEventListener('DOMContentLoaded', () => {
            document.querySelectorAll('p').forEach(el => {
                if(el.querySelectorAll('img').length > 1) {
                    el.classList.add('gallery')
                }
            })
        })

    </script>
</body>
</html>
